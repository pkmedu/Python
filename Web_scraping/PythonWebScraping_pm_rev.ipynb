{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d672f36-2082-4717-bb3d-6e2dcff0ef97",
   "metadata": {},
   "source": [
    "Acknowlegements: This Python script  below was originally presented at the workshop (Python Web Scraping) by Erich Purpur at UVA (10/4/2022). \n",
    "\n",
    "https://github.com/epurpur/PythonWebScraping/blob/master/PythonWebScraping.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a347499-96da-4555-a8d9-31c16ea6b11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\pmuhuri'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4fdae8-743a-44e9-b0dd-763e59e44a65",
   "metadata": {},
   "source": [
    "## Intro to Web Scraping in Python\n",
    "\n",
    "Today we will learn how to scrape HTML web pages in python, using the Beautiful Soup 4 library. We can programmatically gather information from websites to use for your own purposes. We will gather information about the UVA basketball team, first by walking through each step of the process. Then by doing it in a more automated way.\n",
    "\n",
    "First we need to install the Beautiful Soup 4 and lxml libraries. These are not a part of base python or the Anaconda installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13f47c27-5ebe-49b4-817f-17c089bf40ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install --yes beautifulsoup4\n",
    "!conda install --yes lxml\n",
    "!conda install --yes requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78ef6a8-04a5-4120-a68a-1eda4ac3cc79",
   "metadata": {},
   "source": [
    "Next, let's import the libraries we will be using in this Jupyter Notebook, including Beautiful Soup 4. The other two, requests and pandas, are already installed if you are using Anaconda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d788f95-4dbd-4179-ad38-2d0dbd7fe7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d610fe-d1cd-4cb3-b5e2-cdac8286f569",
   "metadata": {},
   "source": [
    "Now we are ready to make an HTTP request using the requests library. This means that once you have established a connection with the destination (the server which hosts the website you want to communicate with), the client (you) sends an HTTP GET request to the server to retrieve the website and all data within it.\n",
    "\n",
    "This is typically done by your web browser, but we can also do it in python.\n",
    "\n",
    "Today we are going to scrape information about players on the UVA men's basketball team. You can find the team's roster for the 2021-2022 season here: https://virginiasports.com/sports/mbball/roster/\n",
    "\n",
    "We will start by gathering information about one of UVA's players, Kihei Clark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "244f77da-f9e0-482e-9ce5-7accf58b0545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "#This is how to make an HTTP GET request using the requests library.\n",
    "source = requests.get('https://virginiasports.com/sports/mbball/roster/season/2022-23/player/kihei-clark/')\n",
    "\n",
    "print(source)      #this prints the type of response. 200 means \"OK\". There are many response codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad98e563-329d-42ec-9a8c-4120f9049d03",
   "metadata": {},
   "source": [
    "Now, we have a response from the server. If the response is good, the source code of the web page is contained within that response. Let's see what that looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78d4f185-523b-426d-b9d4-8c83e13be701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(source.text, 'html.parser')\n",
    "print(type(soup))\n",
    "#print(soup) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9bb70-9728-465a-9f28-765a36c567be",
   "metadata": {},
   "source": [
    "This is messy, but it is all the code for the page we have issued a request for. Some of it is human readable, some of it is not. Now, let's look at the source code of this page another way. Copy and paste this link into your web browser: https://virginiasports.com/sports/mbball/roster/season/2022-23/player/kihei-clark/\n",
    "\n",
    "#### Note - You need to use Google Chrome to have access to inspector and other developer tools\n",
    "\n",
    "Right click somewhere on your page and click \"inspect\". Then, make sure to choose the 'elements' tab to see the HTML source code of this page. While inspecting the page elements, you can see which parts of the page are controlled by different parts of the code. Notice that the code starts with large chunks (< body > for example), and has divisions within that (< div > tags), among others.\n",
    "\n",
    "The class \"bio-info\" looks like it contains the majority of the information in the body of the page. Let's start with this. We are going to scrape some information about Kihei Clark from this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5355a25-d50d-4586-bbce-69156d01bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prettify() function makes the code somewhat more readable.\n",
    "# I don't use this feature much but maybe you will appreciate it.\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8117c0-1ba0-4aac-9918-87cb863029b1",
   "metadata": {},
   "source": [
    "The find() function finds the first item matching this criteria. Notice our arguments are first, the HTML tag, and second, the class within that tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b8191d-1c12-4642-8dc8-2e5b8649897a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_info = soup.find(\"div\", class_='bio-info')    #class_, because 'class' is a reserved word in python\n",
    "#print(player_info.prettify())\n",
    "type(player_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cef7816-26bc-4a05-b507-34dac658e676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kihei Clark\n"
     ]
    }
   ],
   "source": [
    "player_name = soup.find('div', class_='bio-info').div.h1.text\n",
    "print(player_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcd5679a-e781-49e0-8df3-3efde3a01b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guard\n",
      "5'10''\n",
      "167 lbs.\n",
      "Graduate Student\n",
      "Woodland Hills, Calif.\n",
      "Taft Charter\n",
      " \n",
      " @ClarkKihei\n",
      " @kihei.clark\n"
     ]
    }
   ],
   "source": [
    "for item in soup.find_all('div', class_=\"value\"):\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5f1d9e9-e459-4f99-82b0-f186e96df944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position\n",
      "Height\n",
      "Weight\n",
      "Class\n",
      "Hometown\n",
      "High School\n",
      "Previous School\n",
      "Twitter\n",
      "Instagram\n"
     ]
    }
   ],
   "source": [
    "for item in soup.find_all('div', class_='description'):\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb889f47-0cbe-46f4-bcd6-f9e5524c3824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Position': 'Guard', 'Height': \"5'10''\", 'Weight': '167 lbs.', 'Class': 'Graduate Student', 'Hometown': 'Woodland Hills, Calif.'}\n"
     ]
    }
   ],
   "source": [
    "player_stats=[]\n",
    "labels=[]\n",
    "\n",
    "for i in soup.find_all('div', class_='value'):\n",
    "    player_stats.append(i.text)\n",
    "    \n",
    "for i in soup.find_all('div', class_='description'):\n",
    "    labels.append(i.text)\n",
    "    \n",
    "#let's assume I only want the first five items in each list\n",
    "player_stats = player_stats[:5]\n",
    "labels = labels[:5]\n",
    "\n",
    "player_dict = dict(zip(labels, player_stats))\n",
    "print(player_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3863c989-c6a4-4dac-9ae5-0421442b7e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woodland Hills, Calif.\n"
     ]
    }
   ],
   "source": [
    "print(player_dict['Hometown'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fce365-00fb-4f26-9e24-7c93b779e9af",
   "metadata": {},
   "source": [
    "## Selenium\n",
    "Now that we have learned to scrape static HTML content, let's automate this task using Selenium. Check out the Selenium documentation here: https://www.selenium.dev/\n",
    "\n",
    "## Install selenium\n",
    "!conda install --yes seleniumSelenium Web Driver\n",
    "\n",
    "In order to use Selenium, we must download and install a web driver which allows you to drive a browser with your code.\n",
    "\n",
    "Important The following code assumes you are using Google Chrome and will use the associated web driver. If you are using another browser (safari, firefox, edge, etc) you will need to download the selenium web driver for that browser. Just check the Selenium documentation in order to do that.\n",
    "\n",
    "You also need to make sure to download the correct web driver which corresponds to your version of Google Chrome.\n",
    "\n",
    "I have included a detailed writeup about this in the 'WebDriverInstall.md' file in the github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1c7da5-45c1-4bb2-bf56-ef103bccec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install selenium\n",
    "!conda install --yes selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "553be001-e75e-424b-be0d-a2c7267b80d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import selenium and webdrivers\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ca98f4-8246-40ec-96bc-3f2b6fbc28c5",
   "metadata": {},
   "source": [
    "Let's make sure now that Selenium is working and you have all your paths set up correctly. Important: You will have to change the path below to the path of 'driver' on your own computer.\n",
    "\n",
    "If this works correctly, it will open up a blank browser with a message that 'this is being controlled by automated test software'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6f96a94-f53c-446a-a874-a79e4920eeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<selenium.webdriver.chrome.webdriver.WebDriver (session=\"bd171782316ec5c5b670cd45f3a9522d\")>\n"
     ]
    }
   ],
   "source": [
    "# Make sure that Selenium is working and you have all your 'MY_PATH' variable set up correctly\n",
    "# example: /Users/ep9k/Desktop/PythonWebScraping-master/chromedriver\n",
    "MY_PATH = \"C:/PythonWebScraping-master/chromedriver\"\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=MY_PATH)\n",
    "print(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d29794-baf5-42d9-9fe5-2c0363fb7b1b",
   "metadata": {},
   "source": [
    "Now we can go directly to a page of our choice like so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e22d314b-a387-4597-813f-11de2b135763",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path=MY_PATH)\n",
    "driver.get('https://virginiasports.com/sports/mbball/roster/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01dbf72-aeaa-41ee-aeb6-00a03117c2ab",
   "metadata": {},
   "source": [
    "We can now proceed to write out script just like any other program. Just like BeautifulSoup, Selenium provides the ability to select HTML elements by the tag name, class name, id name, and so on.\n",
    "\n",
    "On the UVA roster homepage, looking at the HTML you can see that each player has it's own box with a picture, name, and so on. The HTML is the same for all of those players. In our example, we will simulate that the user is clicking on each player to see that player's individual page. Then we will scrape the information off of that page just like we did in earler. Let's start with just one player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5325030-689e-447c-8416-b38f9924451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path=MY_PATH)\n",
    "driver.get('https://virginiasports.com/sports/mbball/roster/')\n",
    "\n",
    "#I am selecting by x path. xpath can be used to navigate through XML documents\n",
    "player = driver.find_element_by_xpath('//*[@id=\"players\"]/div[1]/div[2]/div[1]')\n",
    "player.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b3363-4470-4d49-8108-2b9439945c49",
   "metadata": {},
   "source": [
    "See above that I used the 'find_element_by_xpath' method to click on this player's name. Let's define what an XPath is.\n",
    "\n",
    "XPath: XPath enables testers to navigate through the XML structure of an HTML or XML document. Don't worry too much about this. I like to use 'find_element_by_xpath' because it is basically a unique identifier for items in the HTML document and makes selecting them easy.\n",
    "\n",
    "To select an element (by XPath or another way), right click on the thing on the page you are interested in, click \"inspect\", then in the console in the \"elements\" tab right click on the HTML of that thing, click \"copy\", click \"XPath\".\n",
    "\n",
    "There are many different ways to select HTML elements in Selenium. Check out the documentation for more examples: https://selenium-python.readthedocs.io/locating-elements.html\n",
    "\n",
    "Now that we have Selenium installed and set up and we did a small example, we can now expand upon it. We will take just a few players from the team and iterate through them. Once we land on each page we will do exactly what we just did with static HTML with BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7663d6ac-e18c-4576-97ba-01af94199686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kihei Clark\n",
      "Guard\n",
      "5'10''\n",
      "167 lbs.\n",
      "Graduate Student\n",
      "Woodland Hills, Calif.\n",
      "Taft Charter\n",
      "\n",
      " @ClarkKihei\n",
      " @kihei.clark\n",
      "\n",
      "Jayden Gardner\n",
      "Forward\n",
      "6'6''\n",
      "233 lbs.\n",
      "Fifth Year\n",
      "Wake Forest, N.C.\n",
      "Heritage\n",
      "East Carolina\n",
      " @Jayy_Baller_1\n",
      " @ssjjaygee\n",
      "\n",
      "Reece  Beekman\n",
      "Guard\n",
      "6'3''\n",
      "190 lbs.\n",
      "Junior\n",
      "Milwaukee, Wis.\n",
      "Scotlandville Magnet (La.)\n",
      " @reece_beekman\n",
      " @reece.2\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(executable_path=MY_PATH)\n",
    "driver.get('https://virginiasports.com/sports/mbball/roster/')\n",
    "#we will use a while loop to grab data about the first three players\n",
    "count = 0\n",
    "\n",
    "while count < 3:\n",
    "    count += 1\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #select player's name using find_element_by_xpath()\n",
    "    #each player's name is a link to their bio page\n",
    "    player = driver.find_element_by_xpath(f'//*[@id=\"players\"]/div[{count}]/div[2]/div[1]/a')\n",
    "                                           # //*[@id=\"players\"]/div[{count}]/div[2]/div[1]/a\n",
    "    player.click()\n",
    "    \n",
    "    #now we use beautiful soup to parse the HTML just as we did last time\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    player_name = soup.find('div', class_='bio-info').div.h1.text\n",
    "    print()\n",
    "    print(player_name)\n",
    "    \n",
    "    for item in soup.find_all('div', class_=\"value\"):\n",
    "        print(item.text) \n",
    "        \n",
    "    #go back to roster page after collecting information about current player    \n",
    "    driver.get('https://virginiasports.com/sports/mbball/roster/')\n",
    "    \n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bec7fc-e9cf-4769-93bb-f03a8a683f74",
   "metadata": {},
   "source": [
    "Let's do just a little more to make it pretty. This time we will collect the information about each player and put it into a pandas dataframe. There are many ways to do this but I will use lists to populate the columns of the dataframe.\n",
    "\n",
    "We will do the whole thing together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bae31e5-e1b0-494a-8725-8a5f90ffd9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kihei Clark', 'Jayden Gardner', 'Reece Beekman', 'Armaan Franklin', 'Ben Vander Plas', 'Taine Murray', 'Isaac McKneely', 'Chase Coleman']\n",
      "['Guard', 'Forward', 'Guard', 'Guard', 'Forward', 'Guard', 'Guard', 'Guard']\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(executable_path=MY_PATH)\n",
    "driver.get('https://virginiasports.com/sports/mbball/roster/')\n",
    "\n",
    "#accumulator lists we will use later to make our pandas dataframe\n",
    "names = []\n",
    "positions = []\n",
    "heights = []\n",
    "weights = []\n",
    "years = []\n",
    "#we will use a while loop to grab data about the first three players\n",
    "count = 0\n",
    "\n",
    "team_players = ['Kihei Clark', 'Jayden Gardner', 'Reece Beekman', 'Armaan Franklin', \n",
    "                'Ben Vander Plas', 'Taine Murray', 'Isaac McKneely', 'Chase Coleman']\n",
    "\n",
    "#there are 14 players on the team\n",
    "for i in team_players:\n",
    "    count += 1\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #select player's name using find_element_by_xpath()\n",
    "    #each player's name is a link to their bio page\n",
    "    player = driver.find_element_by_xpath(f'//*[@id=\"players\"]/div[{count}]/div[2]/div[1]/a')\n",
    "    \n",
    "    player.click()\n",
    "    \n",
    "    #now we use beautiful soup to parse the HTML just as we did last time\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    \n",
    "    #player info will be used to store the data about each player in a list\n",
    "    #this will look like:  ['Kihei Clark', 'Guard', '5'10\"', '167 lbs', 'Senior']\n",
    "    player_info = []\n",
    "    \n",
    "    for item in soup.find_all('div', class_=\"value\"):\n",
    "        player_info.append(item.text)\n",
    "        \n",
    "    #add the information from player_info to the accumulator lists outside this loop\n",
    "    names.append(i)    # i is player name from team_players\n",
    "    positions.append(player_info[0])\n",
    "    heights.append(player_info[1])\n",
    "    weights.append(player_info[2])\n",
    "    years.append(player_info[3])\n",
    "    \n",
    "    #go back to roster page after collecting information about current player    \n",
    "    driver.get('https://virginiasports.com/sports/mbball/roster/')\n",
    "    \n",
    "driver.quit()\n",
    "\n",
    "print(names)\n",
    "print(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcb00a36-b147-42a5-8368-a7a179cf0b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Name Position  Height    Weight              Year\n",
      "0      Kihei Clark    Guard  5'10''  167 lbs.  Graduate Student\n",
      "1   Jayden Gardner  Forward   6'6''  233 lbs.        Fifth Year\n",
      "2    Reece Beekman    Guard   6'3''  190 lbs.            Junior\n",
      "3  Armaan Franklin    Guard   6'4''  200 lbs.            Senior\n",
      "4  Ben Vander Plas  Forward   6'8''  236 lbs.  Graduate Student\n",
      "5     Taine Murray    Guard   6'5''  205 lbs.         Sophomore\n",
      "6   Isaac McKneely    Guard   6'4''  179 lbs.          Freshman\n",
      "7    Chase Coleman    Guard  5'10''  165 lbs.            Senior\n"
     ]
    }
   ],
   "source": [
    "# Make empty dataframe\n",
    "bball_team_df = pd.DataFrame()\n",
    "\n",
    "# Take lists of team information and put them into columns of dataframe\n",
    "bball_team_df['Name'] = names\n",
    "bball_team_df['Position'] = positions\n",
    "bball_team_df['Height'] = heights\n",
    "bball_team_df['Weight'] = weights\n",
    "bball_team_df['Year'] = years\n",
    "print(bball_team_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144e91b1-9f97-4413-a520-493a4f46a106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6951ce62-9491-4b00-82fe-820b1e41884a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
