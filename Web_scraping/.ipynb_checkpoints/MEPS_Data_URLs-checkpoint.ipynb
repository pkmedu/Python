{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4011e892-7206-4531-9425-88270b68d5ab",
   "metadata": {},
   "source": [
    "#### Automation to Extract ZIP Data File URLs from the Medical Expenditure Panel Survey Web Link\n",
    "#### Pradip K. Muhuri\n",
    "\n",
    "This program automates the scrapping of the Medical Expenditure Panel Survey’s web link https://meps.ahrq.gov/data_files/pufs/ to extract the ZIP data files’ Uniform Resource Locators (URLs).  The output of this program is an Excel Workbook with multiple worksheets containing clickable URLs grouped by data formats (e.g., SAS V9, SAS transport, Stata, Excel, and ASCII formats) for easy ZIP file downloads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10bd3c80-7863-4ec8-9df4-943379e5c517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1,349 Full URLs for 5 format-specific data files on the MEPS Website.\n",
      "Listing of first 5 URLs\n",
      "https://meps.ahrq.gov/data_files/pufs/h01dat.zip\n",
      "https://meps.ahrq.gov/data_files/pufs/h036/h36dta.zip\n",
      "https://meps.ahrq.gov/data_files/pufs/h036/h36u19dat.zip\n",
      "https://meps.ahrq.gov/data_files/pufs/h036/h36u19dta.zip\n",
      "https://meps.ahrq.gov/data_files/pufs/h036/h36u19ssp.zip\n"
     ]
    }
   ],
   "source": [
    "# Step 1: List all MEPS data file URLs \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "full_url_list = []\n",
    "tuple_values = 'v9.zip', 'ssp.zip', 'dta.zip', 'dat.zip', 'xlsx.zip', '/'\n",
    "def get_links(base_link):\n",
    "    response = requests.get(base_link)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tags = soup.find_all('a')\n",
    "    for tag in tags:\n",
    "        if tag.text.endswith(tuple_values):\n",
    "            href = tag.get_text()\n",
    "            full_url = base_link + href\n",
    "            if href[-1]=='/':\n",
    "                get_links(full_url)        \n",
    "            else:\n",
    "                #print(full_url)\n",
    "                full_url_list.append(full_url)\n",
    "                \n",
    "get_links('https://meps.ahrq.gov/data_files/pufs/')\n",
    "print('There are', f\"{len(full_url_list):,}\", 'Full URLs for 5 format-specific data files on the MEPS Website.')\n",
    "\n",
    "print('Listing of first 5 URLs')\n",
    "for item in full_url_list[:5]:   print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "650deb3d-0ff5-4e42-9406-ecc602f559d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1,349 Full URLs for data files with extensions of interest.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dat.zip     557\n",
       "ssp.zip     557\n",
       "dta.zip      79\n",
       "v9.zip       78\n",
       "xlsx.zip     78\n",
       "Name: sub_string, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Create a dataframe from the list created in the previous step\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "\n",
    "col1 = \"full_url\"\n",
    "df = pd.DataFrame({col1:full_url_list})\n",
    "values = ['v9.zip', 'ssp.zip', 'dta.zip', 'dat.zip', 'xlsx.zip']\n",
    "sub_string = list(map(df['full_url'].str.contains, values))\n",
    "df['sub_string'] = np.select(sub_string, values, 'other')\n",
    "print('There are', f\"{len(df):,}\", 'Full URLs for data files with extensions of interest.')\n",
    "df['sub_string'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d624511c-c06d-4a2d-aef6-074930c9b221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8426e30e-6128-4444-a018-2efde3db8359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create an Excel Workbook with multiple sheets\n",
    "import pandas as pd \n",
    "import xlsxwriter \n",
    "\n",
    "with pd.ExcelWriter('c:\\\\Python\\\\Web_Scraping\\\\MEPS_urls_workbook.xlsx') as writer:\n",
    "    for i, x in df.groupby('sub_string'):\n",
    "        x.drop('sub_string', axis=1).to_excel(writer, sheet_name=i, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afc2c7c3-d17c-47ff-ac57-54c9d9c4b7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Web_scraping\\MEPS_Data_URLs.ipynb\n",
      "C:\\Python\\Web_scraping\\MEPS_urls_workbook.xlsx\n",
      "C:\\Python\\Web_scraping\\PythonWebScraping_pm_rev.ipynb\n",
      "C:\\Python\\Web_scraping\\scrapy_zipfiles.py\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "path = 'C:\\\\Python\\\\Web_Scraping'\n",
    "files = (f for f in glob.glob(path + '**/*.*', recursive=True))\n",
    "for f in files:\n",
    "    print(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
