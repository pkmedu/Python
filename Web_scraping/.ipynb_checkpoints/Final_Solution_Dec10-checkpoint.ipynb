{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd544913-00b9-44b4-a4ef-ea55f5a20db5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439 file names listed in the MEPS website\n"
     ]
    }
   ],
   "source": [
    "# Import required Python libraries\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, re, Comment\n",
    "import pandas as pd  \n",
    "import xlsxwriter\n",
    "\n",
    "# Step 1: Scraping the primary \"MEPS data file website\", \n",
    "# finding the data file names that are within the \"option\"  \n",
    "# comment tags, and saving them in a csv file\n",
    "\n",
    "def extractOptions(inputData):\n",
    "    sub1 = str(re.escape('<option value=\"All\">All data files</option>'))\n",
    "    sub2 = str(re.escape('</select>'))\n",
    "    result = re.findall(sub1+\"(.*)\"+sub2, inputData, flags=re.S)\n",
    "    if len(result) > 0:\n",
    "        return result[0]\n",
    "\n",
    "def extractData(inputData):\n",
    "    sub1 = str(re.escape('>'))\n",
    "    sub2 = str(re.escape('</option>'))\n",
    "    result =  re.findall(sub1+\"(.*)\"+sub2, inputData, flags=re.S)\n",
    "    if len(result) > 0:\n",
    "        return result[0]\n",
    "    return ''\n",
    "\n",
    "def main(base_url):\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "\n",
    "    for c in comments:\n",
    "        if '<select id=\"pufnumber\" size=1 name=\"cboPufNumber\">' in c:\n",
    "            options = extractOptions(c)\n",
    "            ops = options.splitlines() #split text into lines\n",
    "            fp = open(r'C:/Data/MEPS_fn.csv', 'w')\n",
    "            for op in ops:\n",
    "                data = extractData(op)\n",
    "                if data != '': #check if the data found\n",
    "                    fp.write(data +'\\n')                    \n",
    "            fp.close()    \n",
    "            \n",
    "            with open(r'C:/Data/MEPS_fn.csv', 'r') as buff:\n",
    "                for i, line in enumerate(buff, 1):\n",
    "                    pass\n",
    "                print(f\"{(i)}\", 'file names listed in the MEPS website') \n",
    "                \n",
    "main('https://meps.ahrq.gov/data_stats/download_data_files.jsp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c99e5f3d-d470-400e-b791-0d04c760d7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402 MEPS public-use file names\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Creating a Pandas DataFrame from the csv file 12/31/2022\n",
    "\n",
    "colname = ['file_name']\n",
    "df1 = pd.read_csv(r'C:/Data/MEPS_fn.csv',  sep='\\t', names = colname)\n",
    "\n",
    "df1.drop(df1[df1['file_name'].str.contains('replaced|CD-ROM|NHC|NHEA|NHIS Link|HC-IC Linked| \\\n",
    "1996 Parent IDs')].index, inplace=True)\n",
    "            \n",
    "df1[\"file_id\"] = df1[\"file_name\"].str.extract(r\"([A-Z])[A-Z]+-(\\d+[A-Z]*)\").sum(axis=1).str.lower()\n",
    "df1['file_id'] = df1['file_id'].str.replace('h0', 'h').str.replace('h36', 'h036') \\\n",
    ".str.replace('h36brr', 'h036brr')\n",
    "\n",
    "df1[\"url1\"] = \"https://meps.ahrq.gov/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-\" \\\n",
    "+ df1[\"file_name\"].str.extract(r\"(\\d+[A-Z]*)\").sum(axis=1).astype(str)\n",
    "\n",
    "df1.reset_index(drop = True, inplace = True)\n",
    "\n",
    "print(\"{:,}\".format(len(df1)), 'MEPS public-use file names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9574640a-b1f3-4928-a4f6-9cbc7fbce349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,154 URLs that are specific to data file formats\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Scraping all MEPS data file-specific websites \n",
    "# and saving format-spcific file names from each of\n",
    "# those sites in a DataFrame 12/31/2022\n",
    "\n",
    "url2_str_list = []\n",
    "for item in df1.index:\n",
    "    url1_str = df1['url1'][item]\n",
    "    response = requests.get(url1_str)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    for link in soup.find_all('a'):\n",
    "        if link.text.endswith('ZIP'):\n",
    "            url2_str = 'https://meps.ahrq.gov' + link.get('href').strip('..')\n",
    "            url2_str_list.append(url2_str)\n",
    "\n",
    "df2  = pd.DataFrame(url2_str_list, columns=['url2'])            \n",
    "df2['file_id'] = df2['url2'].str.extract(r\"([h]\\d+[abcdefghir]*(?!\\d))\").sum(axis=1)\n",
    "df2['file_id'] = df2['file_id'].str.replace('da', '')\n",
    "\n",
    "df1 = df1.drop('url1',axis=1)    \n",
    "merged_df = pd.merge(df1, df2, on='file_id', validate =\"one_to_many\")  \n",
    "print(\"{:,}\".format(len(merged_df)), 'URLs that are specific to data file formats')\n",
    "with pd.ExcelWriter('merged_df.xlsx') as writer:\n",
    "    merged_df.to_excel(writer, sheet_name='data_urls', index=False)\n",
    "    writer.sheets['data_urls'].set_column(45, 3, 45)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
